{"cells":[{"cell_type":"markdown","source":["Student Name: MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI\n","\n","Student Number: A194789"],"metadata":{"id":"ATXCYT3j704N"}},{"cell_type":"markdown","source":["# Answer 4 question in this google colab."],"metadata":{"id":"FSlFgmIG79NC"}},{"cell_type":"markdown","source":["## Phase 1: Dataset collection\n","1. Download Harry Potter books from UKMFolio (Information Extraction Lab Activity).\n","2. Open google drive. Create folder TP2043 -> Dataset -> HarryPotter\n","3. Save the books into HarryPotter folder."],"metadata":{"id":"Z0ufyrDQaJLz"}},{"cell_type":"markdown","source":["Link Google Colab to Google Drive"],"metadata":{"id":"1ecyGFzOaPbR"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"MjiaWToAQEIX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715435525145,"user_tz":-480,"elapsed":51877,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}},"outputId":"0c5a7c15-4b8a-4703-88e1-8b1c2ef7d85d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["We will use pandas to present our dataset."],"metadata":{"id":"pCSXK9DmbjeG"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"wsM8oeEIQK8e","executionInfo":{"status":"ok","timestamp":1715435605507,"user_tz":-480,"elapsed":461,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","source":["Read directory to access Harrry Potter books"],"metadata":{"id":"yPgDMWibbplQ"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715435607958,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"hOmRdbmnQMm8","outputId":"a0860a25-9e38-4bae-87d4-8282e43268ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/My Drive/TP2043/Dataset/HarryPotter'\n","/content\n"]}],"source":["%cd /content/drive/My Drive/TP2043/Dataset/HarryPotter"]},{"cell_type":"markdown","source":["Open and reed content from HarryPotter folder. In this example, we will read Book 1. Observe the content and characteristics of the book. You will need to plan preprocessing processes based on the characteristics of your input."],"metadata":{"id":"zpt1bQvkbwl9"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"2yTPxO3bQONh","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1715435615353,"user_tz":-480,"elapsed":465,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}},"outputId":"5acff494-94b9-4bf2-c3b0-c366d8bb60d7"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'book_1.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-68d77f4635c2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"book_1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcontent_book1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontent_book_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_book1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_book1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'book_1.txt'"]}],"source":["file = open(\"book_1.txt\", \"r\")\n","content_book1 = file.read()\n","content_book_raw = content_book1\n","print(content_book1)\n","file.close()"]},{"cell_type":"markdown","source":["## Phase 2 Preprocessing 1: Lower case"],"metadata":{"id":"17fsaTwKc78u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoXgSBzxQZmV"},"outputs":[],"source":["content_book1 =content_book1.lower()\n","content_book1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tg04flCjTpP1"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","source":["# Phase 2 Preprocessing 2: Tokenization and Stop word removal"],"metadata":{"id":"XKBpG5fzdFbo"}},{"cell_type":"markdown","source":["1. Tokenize word in the book. In this example, we will use regular expression. Pattern \"\\w+\" is used to identify words and other character such as punctuation, numbers etc will be ignored."],"metadata":{"id":"nsW1mEa4ecTz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXsCeH3BUaBp"},"outputs":[],"source":["from nltk.tokenize import RegexpTokenizer\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","words_list_book1 = tokenizer.tokenize(content_book1)"]},{"cell_type":"markdown","source":["2. We also can calculate how many words in the book before we do the stopword removal."],"metadata":{"id":"gYb_bFk-eh_7"}},{"cell_type":"code","source":["len(words_list_book1)"],"metadata":{"id":"_6NoXL-IeXA0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Next, we will remove stop words from the \"words_list_book1\". The following code will download stopwords list from NLTK library. You may add additional words to improve you stopword removal."],"metadata":{"id":"OOyJ4kbOfLOB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaBsoHPmVUiB"},"outputs":[],"source":["\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","stop_words.add(\"\") # you may add additional stopwords to clean your data.\n","\n","\n"]},{"cell_type":"markdown","source":["4. We will use counter to calculate word frequencies for each word inside the book. To use counter, we will use Counter library."],"metadata":{"id":"aWgpLrR3dt0H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmffIzhwUDey"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"markdown","source":["5. We will call our counter \"freq\". Add the tokenized input (\"words_list_book1\") inside the Counter as follows. After that, observe the most common words. In this example, we will observe 20 most common words in the book. You may change the value accordingly. We havent remove the stopword yet."],"metadata":{"id":"TvU5hlEvgIV7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDVEAazlT0i-"},"outputs":[],"source":["freq = Counter(words_list_book1)\n","freq.most_common(20)"]},{"cell_type":"markdown","source":["# Question 1 Observation before implementing stop word removal:\n","Analyse 3 Harry Potter books. Based on your observation, what are the most common words used in Harry Potter books? Provide some justifications why the words are common in the observed books?"],"metadata":{"id":"Rb9rlq6vhJ-c"}},{"cell_type":"markdown","source":["6. Next, we will remove the stopwords from the list. We will call our clean words as \"filtered_word_list_book1\"."],"metadata":{"id":"ZEMKXBhHiFDa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPMU8yWTVdKX"},"outputs":[],"source":["\n","\n","#they are present in stop_words or not\n","filtered_word_list_book1 = [w for w in words_list_book1 if not w.lower() in stop_words]\n","#with no lower case conversion\n","filtered_word_list_book1 = []\n","\n","for w in words_list_book1:\n","    if w not in stop_words:\n","        filtered_word_list_book1.append(w)\n","\n","print(\"before removing stopwords\",words_list_book1)\n","print(\"after removing stopwords\", filtered_word_list_book1)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"kIPcO6MliZM7"}},{"cell_type":"markdown","source":["7. We will calculate the new word frequencies based on the new \"filtered_word_list_book1\". You may update your stopwords in (3) to improve your analysis. For example, you may want to remove the word \"said\" from the word list. Our dataset contains many conversations; thus, the word \"said\" occurs highly."],"metadata":{"id":"a-2xAa15iiUQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4cdfOC1Wrtw"},"outputs":[],"source":["freq = Counter(filtered_word_list_book1)\n","freq.most_common(20)"]},{"cell_type":"markdown","source":["8. Try to calculate the number of words inside you dataset after you have cleaned them. Use \"len\" function to calculate number of item in a list. Compare these value - before stopword removal and after stopword removal. This is called \"Dimensionality reduction\" -> to reduce the number of input features in a dataset."],"metadata":{"id":"n-r3bic5l2A5"}},{"cell_type":"code","source":["len(filtered_word_list_book1)"],"metadata":{"id":"XDdB1EuCmJrQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 2: What is the importance of dimensionality reduction in text analytics?"],"metadata":{"id":"HuiUNvY8nBk2"}},{"cell_type":"markdown","source":["9. You have done this before - Word Cloud. We will represent the data in the form of word cloud."],"metadata":{"id":"ap9gVpPdi4Cw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OSSyT56XWvF"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from wordcloud import WordCloud,STOPWORDS\n","book1=(\" \").join(filtered_word_list_book1)\n","wordcloud = WordCloud(max_font_size=60).generate(book1)\n","plt.figure(figsize=(16,12))\n","# plot wordcloud in matplotlib\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"markdown","source":["##Phase 3: Feature Extraction (Bag of Words/ Word Frequency / Unigram)"],"metadata":{"id":"uKPnAFcgovb3"}},{"cell_type":"markdown","source":["1. We can the words in the word frequecy list as text features. We can call the word frequency list as bag of word or unigram. We also can represent our data in the form of histogram graph. The X value is list of words, and the Y value is word's frequency"],"metadata":{"id":"pkqm6RYAjIn0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBdroMjlYlct"},"outputs":[],"source":["y = [count for tag, count in freq.most_common(20)]\n","x = [tag for tag, count in freq.most_common(20)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZiqGjCuZPQE"},"outputs":[],"source":["plt.bar(x, y, color='crimson')\n","plt.title(\"Term frequencies in Book1\")\n","plt.ylabel(\"Frequency (log scale)\")\n","plt.yscale('log') # optionally set a log scale for the y-axis\n","plt.xticks(rotation=90)\n","for i, (tag, count) in enumerate(freq.most_common(20)):\n","    plt.text(i, count, f' {count} ', rotation=90,\n","             ha='center', va='top' if i < 10 else 'bottom', color='white' if i < 10 else 'black')\n","plt.xlim(-0.6, len(x)-0.4) # optionally set tighter x lims\n","plt.tight_layout() # change the whitespace such that all labels fit nicely\n","plt.show()"]},{"cell_type":"markdown","source":["2. You also can observe the word's frequency for specific word as follows. How many time \"harry\" appears in the text?"],"metadata":{"id":"DBMIime9nnww"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2dhnYIya3RP"},"outputs":[],"source":["# Word frequency\n","word_frequency = freq[\"harry\"]  # Frequency of a specific word\n","print(word_frequency)\n","\n"]},{"cell_type":"markdown","source":["3. We also can observe number of unique words in our dataset."],"metadata":{"id":"_YbUAjJGoAqs"}},{"cell_type":"code","source":["# Total unique words\n","unique_words_count = len(freq)\n","print(unique_words_count)"],"metadata":{"id":"3zJhDk9AoPDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sort the words by frequency in ascending order\n","sorted_word_counts = sorted(freq.items(), key=lambda x: x[1])\n","\n","# Find the least common frequency\n","least_common_frequency = sorted_word_counts[0][1]\n","\n","# Extract the least common words\n","least_common_words = [(word, freq1) for word, freq1 in sorted_word_counts if freq1 == least_common_frequency]\n","\n","print(\"Least common words and their frequencies:\")\n","for word, freq1 in least_common_words:\n","    print(f\"{word}: {freq1}\")"],"metadata":{"id":"U1Kwy02xXF-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word count - normalization\n","\n","Word count normalization is a process used in natural language processing (NLP) to adjust the counts of words in a text document to mitigate the effect of document length on analysis. It aims to make comparisons between documents fairer by accounting for differences in length. In this example, we will use frequency-based normalization -dividing the count of each word by the total number of words in the document. This gives the frequency of occurrence of each word relative to the document's length."],"metadata":{"id":"zUsEqmx1p2Yz"}},{"cell_type":"markdown","source":["1. How many words in the clean document? (after preprocessing)"],"metadata":{"id":"j8eFOpiLqaOk"}},{"cell_type":"code","source":["total_tokens = len(filtered_word_list_book1)"],"metadata":{"id":"v9mIyAQoc5FL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. We will normalize each word in the word \"freq\" as follows:"],"metadata":{"id":"oKlL7dfNq0t5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgajX7O1bT6d"},"outputs":[],"source":["\n","normalized_word_counts = {word: count/total_tokens for word, count in freq.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-m26tjEboo3"},"outputs":[],"source":["print(normalized_word_counts)"]},{"cell_type":"markdown","source":["## Phase 3 Feature Extraction (NGram)\n","N-grams are contiguous sequences of n items (usually words or characters) in a text document. The \"n\" in n-grams refers to the number of items in the sequence. So, for example, in a text document, a 2-gram (also known as bigram) would be a sequence of two consecutive words, a 3-gram (trigram) would be a sequence of three consecutive words, and so on."],"metadata":{"id":"yEQsccuJrMx7"}},{"cell_type":"markdown","source":["### Bigram - 2 words"],"metadata":{"id":"DVbytBxYtsbN"}},{"cell_type":"markdown","source":["1. We will use Ngrams library from NLTK to assist our activity. In this example, we will create bigram. Add the clean books in the \"nltk.bigrams(filtered_word_list_book1)\""],"metadata":{"id":"dI8TnQw-rmyU"}},{"cell_type":"code","source":["from nltk import ngrams\n","\n","bigrams = nltk.bigrams(filtered_word_list_book1)"],"metadata":{"id":"KML4GWlNNsPm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Then, add our bigram to Counter to count the bigram frequency."],"metadata":{"id":"Da0tp77Rs0hh"}},{"cell_type":"code","source":["\n","# Count the bigrams\n","bigram_counts = Counter(bigrams)\n","\n","# Print the result\n","print(bigram_counts)\n"],"metadata":{"id":"QmY4_LcZOrZC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Observe top 20 bigrams in the dataset."],"metadata":{"id":"CchS0eoqs-2W"}},{"cell_type":"code","source":["bigram_counts.most_common(20)"],"metadata":{"id":"O7I3rMJzRKzJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"FKgFH_rHtK6u"}},{"cell_type":"code","source":["\n","\n","# Generate bigrams\n","bigrams = list(nltk.bigrams(filtered_word_list_book1))\n","\n","# Count the bigrams\n","bigram_counts = Counter(bigrams)\n","\n","# Extract bigram and frequency data for plotting\n","bigram_data = [(bigram, freq) for bigram, freq in bigram_counts.items()]\n","\n","# Sort the bigram data based on frequency\n","sorted_bigram_data = sorted(bigram_data, key=lambda x: x[1], reverse=True)\n","\n","# Extract bigrams and frequencies for plotting\n","bigrams, frequencies = zip(*sorted_bigram_data)\n","\n","# Select top 20 bigrams and frequencies\n","top_bigrams = bigrams[:20]\n","top_frequencies = frequencies[:20]\n","\n","# Plot the bar chart for top 20 bigrams\n","plt.figure(figsize=(10, 6))\n","plt.bar(range(len(top_bigrams)), top_frequencies)\n","plt.xlabel('Bigrams')\n","plt.ylabel('Frequency')\n","plt.title('Top 20 Bigram Frequency in Text')\n","plt.xticks(range(len(top_bigrams)), top_bigrams, rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"f5hD3rrWVI-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Trigram: 3 words"],"metadata":{"id":"zHUcOr0ttpWC"}},{"cell_type":"code","source":["# Generate bigrams\n","trigrams = list(nltk.trigrams(filtered_word_list_book1))\n","\n","# Count the bigrams\n","trigram_counts = Counter(trigrams)\n","\n","# Extract bigram and frequency data for plotting\n","trigram_data = [(trigram, freq) for trigram, freq in trigram_counts.items()]\n","\n","# Sort the bigram data based on frequency\n","sorted_trigram_data = sorted(trigram_data, key=lambda x: x[1], reverse=True)\n","\n","# Extract bigrams and frequencies for plotting\n","trigrams, frequencies = zip(*sorted_trigram_data)\n","\n","# Select top 20 bigrams and frequencies\n","top_trigrams = trigrams[:20]\n","top_frequencies = frequencies[:20]\n","\n","# Plot the bar chart for top 20 bigrams\n","plt.figure(figsize=(10, 6))\n","plt.bar(range(len(top_trigrams)), top_frequencies)\n","plt.xlabel('Bigrams')\n","plt.ylabel('Frequency')\n","plt.title('Top 20 Bigram Frequency in Text')\n","plt.xticks(range(len(top_trigrams)), top_trigrams, rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"cQfsqsWBVkfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 3:\n","Observe output form word frequency (unigram), bigram and trigram analysis. Describe information that you can capture from the analysis? What are the advantages of using the ngram techniques to extract information from the text?"],"metadata":{"id":"carvEGn4u8ik"}},{"cell_type":"markdown","source":["# Phase 3 Feature Extraction (Yake)\n","1. let test yake, rake and textrank with simple sentences.\n","\n"],"metadata":{"id":"ihLjDsfFvm7_"}},{"cell_type":"code","source":["text = \"\"\"\n","Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence\n","concerned with the interactions between computers and human language, in particular how to program computers\n","to process and analyze large amounts of natural language data. NLP enables computers to understand,\n","interpret, and generate human language in a way that is both useful and meaningful.\n","\"\"\""],"metadata":{"id":"Y_6egJ7f6maN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. then use our dataset, \"content_book_raw\""],"metadata":{"id":"nj5eC4y16oJO"}},{"cell_type":"markdown","source":["1. We need to install yake library into our system."],"metadata":{"id":"gXY_z69YvwHN"}},{"cell_type":"code","source":["!pip install yake"],"metadata":{"id":"Pfu7vWSujt1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Setup parameters - do you own investigation on how to specify parameters in Yake."],"metadata":{"id":"eG5KRSrGv3Ix"}},{"cell_type":"code","source":["import yake\n","\n","# Input Text\n","\n","# Specifying Parameters\n","language = \"en\"\n","max_ngram_size = 3\n","deduplication_thresold = 0.9\n","deduplication_algo = 'seqm'\n","windowSize = 1\n","numOfKeywords = 20\n","\n","\n","custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n","keywords = custom_kw_extractor.extract_keywords(text)#(content_book_raw)\n","\n","for kw in keywords:\n","    print(kw)"],"metadata":{"id":"iYx004PSj0qd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Phase 3 Feature Extraction (Rake)"],"metadata":{"id":"c2JjDOEAwlyg"}},{"cell_type":"markdown","source":["We need to install rake library to our system."],"metadata":{"id":"d-7Hw1Kown-o"}},{"cell_type":"code","source":["!pip install rake-nltk gensim"],"metadata":{"id":"3B4JLU2jmP6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import rake_nltk\n","\n","# Initialize RAKE\n","r = rake_nltk.Rake()\n","#book1_1 = (\" \").join(word_list_book1)\n","\n","# Preprocess the text\n","#r.extract_keywords_from_text(content_book1)\n","r.extract_keywords_from_text(text)#(content_book_raw)\n","\n","stopwords = set(stopwords.words('english'))\n","r.stopwords = stopwords\n","\n","# Get the ranked phrases\n","keywords_rake = r.get_ranked_phrases()\n","\n","# Print the keywords\n","print(keywords_rake)\n","\n","# Get the ranked phrases with scores\n","ranked_phrases = r.get_ranked_phrases_with_scores()\n","\n","# Sort the ranked phrases by score\n","sorted_ranked_phrases = sorted(ranked_phrases, key=lambda x: x[0], reverse=True)\n","\n","\n","# Get the ranked phrases with scores\n","ranked_phrases = r.get_ranked_phrases_with_scores()\n","\n","# Sort the ranked phrases by score\n","sorted_ranked_phrases = sorted(ranked_phrases, key=lambda x: x[0], reverse=True)\n","\n","# Get the top 20 ranked phrases\n","top_20_ranked_phrases = sorted_ranked_phrases[:20]\n","\n","# Print the top 20 ranked phrases\n","for phrase, score in top_20_ranked_phrases:\n","    print(f\"{phrase}: {score}\")"],"metadata":{"id":"09bC0qTgolyy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Phase 3 Feature Extraction (PyTextRank)"],"metadata":{"id":"mtbHFMqzxlEL"}},{"cell_type":"code","source":["import spacy\n","import pytextrank\n","\n","\n","# load a spaCy model, depending on language, scale, etc.\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# add PyTextRank to the spaCy pipeline\n","nlp.add_pipe(\"textrank\")\n","doc = nlp(text)#(content_book_raw)\n","\n","# examine the top-ranked phrases in the document\n","#for phrase in doc._.phrases:\n","#   print(phrase.text, phrase.rank, phrase.count)"],"metadata":{"id":"ruXt_c2Cu312"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the top-ranked phrases\n","top_phrases = [phrase.text for phrase in doc._.phrases[:20]]  # Get top 5 phrases\n","\n","# Print the top phrases\n","for phrase in top_phrases:\n","    print(phrase)"],"metadata":{"id":"OEZLa0Ysx9Hc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Question 4:\n","Compare the information extracted by using Yake, Rake and TextRank based on the simple sentences. Based on your observation, which algorithm works the best to extract the important information from the text and why?"],"metadata":{"id":"aTwBktl67N_z"}}],"metadata":{"colab":{"provenance":[{"file_id":"1V_s6mUhcNGu2S2JPDdzRikWUVKX_S1Cg","timestamp":1715435362049},{"file_id":"1sWIrKghPanBmSR7sqsVTxjcs1XthLE75","timestamp":1714453668367}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}