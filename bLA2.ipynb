{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9EVBcOw+BBtSpuGod0WhF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-G_46qm7ta2_","executionInfo":{"status":"ok","timestamp":1718032394694,"user_tz":-480,"elapsed":12145,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}},"outputId":"64cebc03-4120-40e9-cd83-392e4bb3a868"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.1)\n","Installing collected packages: emoji\n","Successfully installed emoji-2.12.1\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvJFEMqPs4rl","executionInfo":{"status":"ok","timestamp":1718032988942,"user_tz":-480,"elapsed":1797,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}},"outputId":"cc70b17a-8e62-429d-b1c2-f62b6837aee7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Column 'Tweet' not found in DataFrame\n","Column 'Tweet' not found in DataFrame\n","Column 'Tweet' not found in DataFrame\n","Column 'clean' not found in DataFrame\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import emoji\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Install NLTK data if not installed\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load initial CSV files\n","train_df = pd.read_csv('TEC.csv', encoding='utf-8', sep=\"\\t\")\n","dev_df = pd.read_csv('goemotions_full.csv', encoding='utf-8', sep=\"\\t\")\n","test_df = pd.read_csv('semeval2018-task1-emoc.csv', encoding='utf-8', sep=\"\\t\")\n","\n","# Preprocess data\n","def preprocess_data(df):\n","    # Check if 'Tweet' column exists\n","    if 'Tweet' in df.columns:\n","        # Translate emojis\n","        df[\"clean\"] = df[\"Tweet\"].apply(lambda x: emoji.demojize(x))\n","        # Remove URLs\n","        df[\"clean\"] = df[\"clean\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n","        # Tokenize text\n","        df[\"clean\"] = df[\"clean\"].apply(lambda x: nltk.word_tokenize(x.lower()))\n","        # Remove stopwords and punctuation\n","        stop_words = set(stopwords.words('english'))\n","        df[\"clean\"] = df[\"clean\"].apply(lambda x: [word for word in x if word not in stop_words])\n","        df[\"clean\"] = df[\"clean\"].apply(lambda x: [re.sub(r'['+string.punctuation+']', '', word) for word in x])\n","        df[\"clean\"] = df[\"clean\"].apply(lambda x: [re.sub('\\\\n', '', word) for word in x])\n","        # Remove empty words\n","        df[\"clean\"] = df[\"clean\"].apply(lambda x: [word for word in x if len(word.strip()) > 0])\n","    else:\n","        print(\"Column 'Tweet' not found in DataFrame\")\n","    return df\n","\n","# Apply preprocessing\n","train_df = preprocess_data(train_df)\n","dev_df = preprocess_data(dev_df)\n","test_df = preprocess_data(test_df)\n","\n","# Check if 'clean' column exists\n","if 'clean' in train_df.columns and 'clean' in dev_df.columns:\n","    # Combine train and development data for training\n","    train_texts = train_df['clean'].tolist() + dev_df['clean'].tolist()\n","    # Assuming that the columns for emotions are present in the DataFrame\n","    train_labels = train_df[train_df.columns.intersection([\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"])].values.tolist() + \\\n","                   dev_df[dev_df.columns.intersection([\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"])].values.tolist()\n","\n","    # Tokenization\n","    max_words = 10000\n","    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n","    tokenizer.fit_on_texts(train_texts)\n","    X_train = tokenizer.texts_to_sequences(train_texts)\n","    X_test = tokenizer.texts_to_sequences(test_df['clean'].tolist())\n","\n","    # Padding sequences\n","    maxlen = 100\n","    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n","    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n","\n","    # Convert labels to binary matrix\n","    mlb = MultiLabelBinarizer()\n","    y_train = mlb.fit_transform(train_labels)\n","\n","    # Define the model\n","    model = Sequential()\n","    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=maxlen))\n","    model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n","    model.add(Dense(11, activation='sigmoid'))  # 11 output neurons for 11 emotions\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Train the model\n","    callbacks = [EarlyStopping(patience=3, monitor='val_loss')]\n","    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, callbacks=callbacks)\n","\n","    # Evaluate on test data\n","    predictions = model.predict(X_test)\n","\n","    # Assuming you want to save the predictions to a CSV file\n","    predicted_df = pd.DataFrame(predictions, columns=[\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"])\n","    predicted_df.to_csv('predicted_emotions.csv', index=False)\n","else:\n","    print(\"Column 'clean' not found in DataFrame\")\n"]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Load the training data\n","traindf = pd.read_csv('TEC.csv', encoding='utf-8', sep=\"\\t\")\n","\n","# Define the sentiment categories\n","sentiments = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\n","\n","# Check if the sentiment columns exist in the DataFrame\n","if all(sentiment in traindf.columns for sentiment in sentiments):\n","    # Calculate the frequency of each sentiment category\n","    sentiment_counts = traindf[sentiments].sum()\n","\n","    # Plotting the bar chart\n","    plt.figure(figsize=(10, 6))\n","    sentiment_counts.plot(kind='bar', color='skyblue')\n","    plt.title('Frequency of Sentiment Categories')\n","    plt.xlabel('Sentiment Category')\n","    plt.ylabel('Frequency')\n","    plt.xticks(rotation=45)\n","    plt.show()\n","else:\n","    print(\"Sentiment columns not found in the DataFrame.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJMzyGTlv7_r","executionInfo":{"status":"ok","timestamp":1718033164831,"user_tz":-480,"elapsed":378,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}},"outputId":"d8fdcc85-89df-476d-a7c7-e3e5241078bb"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment columns not found in the DataFrame.\n"]}]}]}