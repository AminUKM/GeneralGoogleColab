{"cells":[{"cell_type":"markdown","metadata":{"id":"5Qug7ZBaO3mR"},"source":["\n","This module is prepared for  SOCIAL MEDIA MINING MADE EASY: PYTHON TEXT ANALYTICS CRASH COURSE WORKSHOP, 25-26 September 2023.\n","\n","Author: Dr Lailatul Qadri Zakaria, Asian Language Processing Lab (ASLAN), Center For Artificial Intelligence Technology (CAIT), Faculty of Information Science and Technology (FTSM), Universiti Kebangsaan Malaysia (UKM).\n","\n","Email: lailatul.qadri@ukm.edu.my\n"]},{"cell_type":"markdown","metadata":{"id":"vVxA7r2KVWR_"},"source":["#TOPIC MODELLING"]},{"cell_type":"markdown","metadata":{"id":"GJLji62YVTMG"},"source":["**Topic modelling** is a **natural language processing (NLP)** approach used in **machine learning and text mining** to identify the underlying topics or themes in a set of texts. It's especially effective for organising, summarising, and comprehending enormous amounts of textual material. The goal of topic modelling algorithms is to automatically detect patterns in text and group similar words and documents together based on their content.\n","\n","One of the most popular methods for topic modeling is **Latent Dirichlet Allocation (LDA)**. LDA assumes that texts are topic mixtures and that subjects are word mixtures. It attempts to reverse-engineer this process in order to discover the subjects and their distributions across a corpus of documents. This is how it works:\n","\n","\n","* LDA assumes that each document in the corpus is a combination of subjects. A news piece, for example, may include 30% on politics, 20% about sports, and 50% about technology.\n","\n","* Topic-Word Distribution: : It is also assumed that each subject is a word combination. A \"politics\" theme, for example, may include terms like \"government,\" \"election,\" and \"policy.\"\n","\n","By analyzing the co-occurrence patterns of words across documents, LDA and similar algorithms can estimate these document-topic and topic-word distributions. Researchers and analysts can then interpret the results to understand the primary themes in the data."]},{"cell_type":"markdown","metadata":{"id":"FVUIyHkWhfsS"},"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2671,"status":"ok","timestamp":1718000967484,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"Uo1Wu5BvVQRA","outputId":"70611d23-7b89-4633-eb1f-f395545a26b3"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"]},{"output_type":"stream","name":"stdout","text":["[['artificial', 'intelligence', 'is', 'intelligence', 'exhibited', 'by', 'machines', 'rather', 'than', 'humans', 'or', 'other', 'animals'], ['the', 'field', 'of', 'ai', 'research', 'defines', 'itself', 'as', 'the', 'study', 'of', 'intelligent', 'agents', 'any', 'device', 'that', 'perceives', 'its', 'environment', 'and', 'takes', 'actions', 'that', 'maximize', 'its', 'chance', 'of', 'success', 'at', 'some', 'goal'], ['the', 'overall', 'research', 'goal', 'of', 'artificial', 'intelligence', 'is', 'to', 'create', 'technology', 'that', 'allows', 'computers', 'and', 'machines', 'to', 'function', 'in', 'an', 'intelligent', 'manner'], ['chemistry', 'is', 'sometimes', 'called', 'the', 'central', 'science', 'because', 'it', 'bridges', 'other', 'natural', 'sciences', 'including', 'physics', 'geology', 'and', 'biology'], ['chemistry', 'includes', 'topics', 'such', 'as', 'the', 'properties', 'of', 'individual', 'atoms', 'and', 'how', 'atoms', 'form', 'chemical', 'bonds', 'to', 'create', 'chemical', 'compounds'], ['chemistry', 'is', 'a', 'branch', 'of', 'physical', 'science', 'that', 'studies', 'the', 'composition', 'structure', 'of', 'atoms', 'properties', 'and', 'change', 'of', 'matter']]\n","[(0,\n","  '0.044*\"atoms\" + 0.044*\"create\" + 0.044*\"chemical\" + 0.028*\"artificial\" + '\n","  '0.028*\"machines\" + 0.028*\"intelligence\" + 0.027*\"properties\" + '\n","  '0.027*\"chemistry\" + 0.027*\"research\" + 0.027*\"intelligent\" + '\n","  '0.027*\"computers\" + 0.027*\"function\" + 0.027*\"allows\" + 0.027*\"technology\" '\n","  '+ 0.027*\"overall\" + 0.027*\"goal\" + 0.027*\"manner\" + 0.027*\"individual\" + '\n","  '0.027*\"includes\" + 0.027*\"topics\"'),\n"," (1,\n","  '0.033*\"chemistry\" + 0.033*\"intelligence\" + 0.032*\"science\" + 0.020*\"atoms\" '\n","  '+ 0.020*\"goal\" + 0.020*\"intelligent\" + 0.020*\"research\" + '\n","  '0.020*\"properties\" + 0.020*\"actions\" + 0.020*\"study\" + 0.020*\"perceives\" + '\n","  '0.020*\"ai\" + 0.020*\"takes\" + 0.020*\"environment\" + 0.020*\"device\" + '\n","  '0.020*\"chance\" + 0.020*\"agents\" + 0.020*\"defines\" + 0.020*\"success\" + '\n","  '0.020*\"maximize\"')]\n"]}],"source":["#Import all the required libararies\n","import gensim\n","import nltk\n","from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.models.coherencemodel import CoherenceModel\n","from pprint import pprint\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","import string\n","\n","#Phase 1: Data Collection\n","documents =[\n","    \"Artificial intelligence is intelligence exhibited by machines, rather than humans or other animals.\",\n","    \"The field of AI research defines itself as the study of intelligent agents any device that perceives its environment and takes actions that maximize its chance of success at some goal.\",\n","    \"The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner.\",\n","    \"Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology.\",\n","    \"Chemistry includes topics such as the properties of individual atoms and how atoms form chemical bonds to create chemical compounds.\",\n","    \"Chemistry is a branch of physical science that studies the composition, structure of atoms, properties and change of matter.\"]\n","\n","#Phase 2: Data Cleaning\n","# Tokenize the documents and create a dictionary\n","cleaned = []\n","for doc in documents:\n","  doc = doc.translate(str.maketrans('', '', string.punctuation))\n","  doc = doc.lower()\n","  cleaned.append(doc)\n","\n","\n","tokenized_documents = [doc.split() for doc in cleaned]\n","#clean tokenized document\n","cleaned_documents=[]\n","for tokenized_document in tokenized_documents:\n","  filtered_sentence = [w for w in tokenized_document if not w.lower() in stop_words]\n","  cleaned_documents.append(filtered_sentence)\n","print(tokenized_documents)\n","dictionary = corpora.Dictionary(cleaned_documents)\n","\n","#Phase 3: Document Representation\n","# Create a corpus (a bag of words representation of the documents)\n","corpus = [dictionary.doc2bow(doc) for doc in cleaned_documents]\n","\n","#Phase 4: Modelling\n","# Build an LDA model\n","lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=5)\n","\n","# Print the topics and their top words\n","pprint(lda_model.print_topics(num_words=20))\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o0SQq__qm_O8"},"source":["Lets us test a new document and see if it can be identified in the similar group correctly."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":846,"status":"ok","timestamp":1718000968327,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"677gXinmwWI1","outputId":"9806244b-63ba-4e24-96dc-2637574f81c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Coherence Score: 0.4180324419606567\n","\n","Perplexity:  -4.589589812984205\n"]}],"source":["# Phase 5: Evaluation\n","from gensim.models import CoherenceModel\n","\n","# Compute coherence score\n","coherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_documents, dictionary=dictionary, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","\n","print('Coherence Score:', coherence_lda)\n","\n","# Calculate and print the perplexity\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus))"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718000968328,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"2dOj0s19m9_e","outputId":"e23f873d-3b21-45bd-9198-78d4bca91742"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Topic distribution for the new doc1:\n","[(0, 0.2686649), (1, 0.7313351)]\n","\n","Topic distribution for the new doc2:\n","[(0, 0.3546966), (1, 0.64530337)]\n"]}],"source":["# Test the model on a new document\n","doc1 = \"Chemistry is the branch of science that deals with the properties, composition, and structure of elements and compounds\"\n","doc2 =\"Machine learning is a area of Artificial intelligence\"\n","\n","new_bow_doc1 = dictionary.doc2bow(doc1.split())\n","print(\"\\nTopic distribution for the new doc1:\")\n","pprint(lda_model[new_bow_doc1])\n","\n","new_bow_doc2 = dictionary.doc2bow(doc2.split())\n","print(\"\\nTopic distribution for the new doc2:\")\n","pprint(lda_model[new_bow_doc2])\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10416,"status":"ok","timestamp":1718000978732,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"p4bsQMmhdGDw","outputId":"cdde5eb7-a122-4ca4-b2ff-a83cc898bcc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyLDAvis\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.0)\n","Collecting funcy (from pyLDAvis)\n","  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"]}],"source":["!pip install pyLDAvis"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":879},"executionInfo":{"elapsed":2992,"status":"ok","timestamp":1718000981716,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"sONxwKALdBPA","outputId":"9724e37f-a6f0-4ab3-ad7a-6f0be52183a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n","\n","\n","<div id=\"ldavis_el3191381175115922242404459114\" style=\"background-color:white;\"></div>\n","<script type=\"text/javascript\">\n","\n","var ldavis_el3191381175115922242404459114_data = {\"mdsDat\": {\"x\": [0.044344045697525494, -0.044344045697525494], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [62.781349394809475, 37.21865060519053]}, \"tinfo\": {\"Term\": [\"create\", \"chemical\", \"atoms\", \"computers\", \"function\", \"allows\", \"technology\", \"overall\", \"manner\", \"individual\", \"includes\", \"topics\", \"form\", \"compounds\", \"bonds\", \"artificial\", \"machines\", \"properties\", \"research\", \"intelligent\", \"goal\", \"science\", \"intelligence\", \"chemistry\", \"actions\", \"study\", \"perceives\", \"ai\", \"takes\", \"environment\", \"science\", \"actions\", \"study\", \"perceives\", \"ai\", \"takes\", \"environment\", \"device\", \"chance\", \"agents\", \"defines\", \"success\", \"maximize\", \"field\", \"matter\", \"change\", \"composition\", \"branch\", \"geology\", \"sometimes\", \"bridges\", \"humans\", \"structure\", \"physical\", \"central\", \"natural\", \"studies\", \"sciences\", \"including\", \"biology\", \"chemistry\", \"intelligence\", \"goal\", \"intelligent\", \"atoms\", \"research\", \"properties\", \"machines\", \"artificial\", \"create\", \"chemical\", \"computers\", \"function\", \"allows\", \"technology\", \"overall\", \"manner\", \"individual\", \"includes\", \"topics\", \"form\", \"compounds\", \"bonds\", \"atoms\", \"artificial\", \"machines\", \"properties\", \"research\", \"intelligent\", \"goal\", \"intelligence\", \"chemistry\", \"called\", \"animals\", \"exhibited\", \"physics\", \"rather\", \"biology\", \"including\", \"science\"], \"Freq\": [1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.4809655781246291, 0.9033275085879651, 0.9031853746225622, 0.9031383380970744, 0.9029903992610124, 0.9029887773118577, 0.9029132286275444, 0.9029007652287764, 0.9028824969593492, 0.9028532165088188, 0.9026223021686296, 0.9025394973959925, 0.9025336925253333, 0.9017409862173865, 0.8944731174207051, 0.8928826682258572, 0.892477607765895, 0.8919386084520488, 0.8910470486114073, 0.8908254391374216, 0.8901327814825992, 0.8897010015444563, 0.8896386845506159, 0.8886818199150596, 0.8883518813106858, 0.8882687350750686, 0.888086735306757, 0.8879653452173858, 0.8877529552438588, 0.8876793699716802, 1.5114128076575608, 1.4986569458471717, 0.9226415937569606, 0.9188063669319413, 0.9379830986172532, 0.9168726620765015, 0.9054084693535069, 0.8919249499328509, 0.8918296817614457, 1.1992186305310546, 1.1918157871073938, 0.727902393419295, 0.7273530506153311, 0.7264744563804515, 0.7264328571372587, 0.7264051243084635, 0.7224444411332791, 0.7210776883827813, 0.7209629615162142, 0.7208807751768649, 0.720712859983539, 0.720025106072358, 0.7186737379568123, 1.206093234416459, 0.7508934651714101, 0.7508157828863724, 0.7398142000677159, 0.7304603926241922, 0.7288825566092694, 0.7257532004023683, 0.7486212456822186, 0.7382133377077169, 0.26756362472305134, 0.2640525625650772, 0.2627271560210132, 0.26242034895062805, 0.2621784205052711, 0.2614248011257015, 0.2613647808064476, 0.27020084973721664], \"Total\": [1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.7511664278618457, 1.1519851123011737, 1.1519588944751056, 1.151950117107591, 1.1519230033239898, 1.1519225453439268, 1.1519087123251532, 1.1519064210041003, 1.151903056599783, 1.151897612211958, 1.1518552102572839, 1.1518398650846176, 1.1518389691270845, 1.1516927970422646, 1.1503549206056638, 1.150062069215057, 1.1499874747624539, 1.1498882280322138, 1.149724098266577, 1.1496833341643062, 1.1495557328961867, 1.1494762813413875, 1.1494648753434742, 1.1492886038060055, 1.1492279216169692, 1.149212538625434, 1.1491790208273507, 1.1491567705401873, 1.1491177360503064, 1.1491041710973817, 2.2496261453652777, 2.2472781915293902, 1.648394794159329, 1.6476889235412107, 2.1440763330337123, 1.6473330547006937, 1.6452226694212229, 1.6427407328192234, 1.6427231469328558, 1.5415889202986177, 1.5432585561792216, 1.0438721954535868, 1.043996196304642, 1.0441943415527164, 1.0442036843178446, 1.0442098203483727, 1.045103161481752, 1.045411455379384, 1.0454374326019915, 1.0454558711345454, 1.0454937940938187, 1.0456487494757107, 1.0459537542515789, 2.1440763330337123, 1.6427231469328558, 1.6427407328192234, 1.6452226694212229, 1.6473330547006937, 1.6476889235412107, 1.648394794159329, 2.2472781915293902, 2.2496261453652777, 1.147718943297345, 1.1485109976126626, 1.1488102719271862, 1.1488795009634678, 1.1489340477504497, 1.1491041710973817, 1.1491177360503064, 1.7511664278618457], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4323, -3.9266, -3.9268, -3.9268, -3.927, -3.927, -3.9271, -3.9271, -3.9271, -3.9271, -3.9274, -3.9275, -3.9275, -3.9284, -3.9365, -3.9382, -3.9387, -3.9393, -3.9403, -3.9406, -3.9413, -3.9418, -3.9419, -3.943, -3.9433, -3.9434, -3.9436, -3.9438, -3.944, -3.9441, -3.4119, -3.4204, -3.9055, -3.9096, -3.889, -3.9117, -3.9243, -3.9393, -3.9394, -3.1204, -3.1266, -3.6197, -3.6204, -3.6217, -3.6217, -3.6217, -3.6272, -3.6291, -3.6293, -3.6294, -3.6296, -3.6306, -3.6324, -3.1147, -3.5886, -3.5887, -3.6035, -3.6162, -3.6183, -3.6226, -3.5916, -3.6056, -4.6205, -4.6337, -4.6387, -4.6399, -4.6408, -4.6437, -4.6439, -4.6107], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.2979, 0.2224, 0.2222, 0.2222, 0.222, 0.222, 0.222, 0.222, 0.2219, 0.2219, 0.2217, 0.2216, 0.2216, 0.2209, 0.2139, 0.2124, 0.212, 0.2115, 0.2106, 0.2104, 0.2098, 0.2093, 0.2093, 0.2084, 0.208, 0.208, 0.2078, 0.2077, 0.2075, 0.2074, 0.0678, 0.0604, -0.1148, -0.1185, -0.3612, -0.1204, -0.1317, -0.1452, -0.1453, 0.7372, 0.7299, 0.6278, 0.627, 0.6256, 0.6255, 0.6255, 0.6191, 0.6169, 0.6168, 0.6166, 0.6164, 0.6153, 0.6131, 0.413, 0.2055, 0.2054, 0.1891, 0.1751, 0.1727, 0.168, -0.1109, -0.1259, -0.4678, -0.4817, -0.487, -0.4882, -0.4892, -0.4922, -0.4925, -0.8805]}, \"token.table\": {\"Topic\": [1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2], \"Freq\": [0.8680667738860163, 0.8681327137051069, 0.8681135780033903, 0.9576761338440127, 0.870692576804782, 0.6087453031066796, 0.6087453031066796, 0.466401305118215, 0.466401305118215, 0.8702431208173329, 0.956065214102644, 0.8696497412720581, 0.8699012769747174, 0.8712934519726973, 0.8701494117833434, 0.8681286105376139, 0.8695182866804071, 0.6479795598708918, 0.8890366090918874, 0.4445183045459437, 0.8695746883735095, 0.9563440883005894, 0.9579716792489876, 0.6486813616993902, 0.8681646713015564, 0.868126074970842, 0.868124348136475, 0.8704657543864492, 0.8682871010118006, 0.9564858305703762, 0.9578578959766595, 0.8697738888031363, 0.6066507875074877, 0.6066507875074877, 0.8699614043649901, 0.9565373965145841, 0.8702328478865473, 0.9565611653232706, 0.44498273679212264, 0.44498273679212264, 0.6069106769564254, 0.6069106769564254, 0.6087387863596889, 0.6087387863596889, 0.956843340309291, 0.8692969292238071, 0.8681769125747196, 0.8701610593251043, 0.9576619377764295, 0.8680931449626312, 0.8701034680831093, 0.870413301970646, 0.6078204601640899, 0.6078204601640899, 0.870371978233168, 0.6070417862049708, 0.6070417862049708, 0.5710479507199031, 0.8702032878681359, 0.8698047282096947, 0.8699700368845005, 0.8701864390807018, 0.8680865305143147, 0.8681762372641418, 0.868113923147005, 0.9576675652636469, 0.9565205262224833], \"Term\": [\"actions\", \"agents\", \"ai\", \"allows\", \"animals\", \"artificial\", \"artificial\", \"atoms\", \"atoms\", \"biology\", \"bonds\", \"branch\", \"bridges\", \"called\", \"central\", \"chance\", \"change\", \"chemical\", \"chemistry\", \"chemistry\", \"composition\", \"compounds\", \"computers\", \"create\", \"defines\", \"device\", \"environment\", \"exhibited\", \"field\", \"form\", \"function\", \"geology\", \"goal\", \"goal\", \"humans\", \"includes\", \"including\", \"individual\", \"intelligence\", \"intelligence\", \"intelligent\", \"intelligent\", \"machines\", \"machines\", \"manner\", \"matter\", \"maximize\", \"natural\", \"overall\", \"perceives\", \"physical\", \"physics\", \"properties\", \"properties\", \"rather\", \"research\", \"research\", \"science\", \"sciences\", \"sometimes\", \"structure\", \"studies\", \"study\", \"success\", \"takes\", \"technology\", \"topics\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n","\n","function LDAvis_load_lib(url, callback){\n","  var s = document.createElement('script');\n","  s.src = url;\n","  s.async = true;\n","  s.onreadystatechange = s.onload = callback;\n","  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n","  document.getElementsByTagName(\"head\")[0].appendChild(s);\n","}\n","\n","if(typeof(LDAvis) !== \"undefined\"){\n","   // already loaded: just create the visualization\n","   !function(LDAvis){\n","       new LDAvis(\"#\" + \"ldavis_el3191381175115922242404459114\", ldavis_el3191381175115922242404459114_data);\n","   }(LDAvis);\n","}else if(typeof define === \"function\" && define.amd){\n","   // require.js is available: use it to load d3/LDAvis\n","   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n","   require([\"d3\"], function(d3){\n","      window.d3 = d3;\n","      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n","        new LDAvis(\"#\" + \"ldavis_el3191381175115922242404459114\", ldavis_el3191381175115922242404459114_data);\n","      });\n","    });\n","}else{\n","    // require.js not available: dynamically load d3 & LDAvis\n","    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n","         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n","                 new LDAvis(\"#\" + \"ldavis_el3191381175115922242404459114\", ldavis_el3191381175115922242404459114_data);\n","            })\n","         });\n","}\n","</script>"]},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x400 with 0 Axes>"]},"metadata":{}}],"source":["import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","\n","# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n","pyLDAvis.display(vis)\n","\n","# Save the visualization as an HTML file (optional)\n","pyLDAvis.save_html(vis, 'lda_visualization.html')\n","\n","# Show the visualization in a matplotlib figure (optional)\n","plt.figure(figsize=(8, 4))\n","pyLDAvis.display(vis)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NvFm2AeAYoL_"},"source":["## Activity: Lets check on topics in Trip Advisor Dataset.\n","\n","\n"]},{"cell_type":"markdown","source":["## Phase 1: Dataset collection\n","\n","In this activity, we will be using Trip Advisor dataset. Lets try to observe topics in the dataset. How many documents do we have in the dataset?\n"],"metadata":{"id":"lWo7y0YBL3ve"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"elapsed":26505,"status":"error","timestamp":1718001008214,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"yaAEk8Xt3yal","outputId":"cbbd6c3a-a33a-4eb1-b159-c162f98fe01d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Detected encoding: MacRoman\n"]},{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: EOF inside string starting at row 8219","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cc9dec1d3e20>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Step 3: Read the CSV content using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 8219"]}],"source":["import chardet\n","import io\n","import pandas as pd\n","\n","file_path = 'trip_advisor_dataset.csv'\n","\n","# Step 1: Detect encoding\n","with open(file_path, 'rb') as file:\n","    raw_data = file.read()\n","\n","result = chardet.detect(raw_data)\n","encoding = result['encoding']\n","print(f\"Detected encoding: {encoding}\")\n","\n","# Step 2: Read the file content with error handling\n","with open(file_path, 'r', encoding=encoding, errors='replace') as file:\n","    content = file.read()\n","\n","# Step 3: Read the CSV content using pandas\n","df = pd.read_csv(io.StringIO(content))\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"ur2lExnRZZL6"},"source":["We will observe the data in cleaned_data column and submit the data to our topic modelling code."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1718001008216,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"314UChAHZ8ir"},"outputs":[],"source":["documents = df['Review']\n","print(documents)"]},{"cell_type":"code","source":["print(documents[10])"],"metadata":{"id":"ibO80_ibp-Wl","executionInfo":{"status":"aborted","timestamp":1718001008216,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Phase 2: Data Preprocessing\n"],"metadata":{"id":"1iJGnNrML-Dq"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1718001008217,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"},"user_tz":-480},"id":"QOg26gocqvE8"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords"]},{"cell_type":"markdown","metadata":{"id":"-hsGQupAZ8Qy"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yNEJi5zZsOJ","executionInfo":{"status":"aborted","timestamp":1718001008217,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["tokenized_documents = [doc.split() for doc in documents]\n","#clean tokenized document\n","cleaned_documents=[]\n","for tokenized_document in tokenized_documents:\n","  filtered_sentence = [w for w in tokenized_document if not w.lower() in stop_words]\n","  cleaned_documents.append(filtered_sentence)\n","print(tokenized_documents)\n","\n","\n","#print(tokenized_documents)\n","dictionary = corpora.Dictionary(cleaned_documents)\n","\n"]},{"cell_type":"markdown","source":["## Phase 3: Data Representation"],"metadata":{"id":"1YKt0AXvMI3v"}},{"cell_type":"code","source":["# Create a corpus (a bag of words representation of the documents)\n","corpus = [dictionary.doc2bow(doc) for doc in cleaned_documents]\n","\n","\n"],"metadata":{"id":"hoxJes6BMHEb","executionInfo":{"status":"aborted","timestamp":1718001008217,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Phase 4: Topic Modelling"],"metadata":{"id":"mwaYgeN4MNH7"}},{"cell_type":"code","source":["\n","# Build an LDA model\n","lda_model = LdaModel(corpus, num_topics=12, id2word=dictionary, iterations=50, passes=8, alpha=1.0)\n","\n","# Print the topics and their top words\n","pprint(lda_model.print_topics(num_words=20))"],"metadata":{"id":"TUPobGb_MIXR","executionInfo":{"status":"aborted","timestamp":1718001008217,"user_tz":-480,"elapsed":11,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Phase 5: Evaluation"],"metadata":{"id":"YL8XnocTMpLc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgsgJDj0JfYc","executionInfo":{"status":"aborted","timestamp":1718001008218,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["from gensim.models import CoherenceModel\n","\n","# Compute coherence score\n","coherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_documents, dictionary=dictionary, coherence='c_v')\n","coherence_lda = coherence_model_lda.get_coherence()\n","\n","print('Coherence Score:', coherence_lda)\n","\n","# Calculate and print the perplexity\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus))"]},{"cell_type":"markdown","source":["## Phase 6: Visualization"],"metadata":{"id":"4bGwlKc8M9yp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnWUwBChbeQI","executionInfo":{"status":"aborted","timestamp":1718001008218,"user_tz":-480,"elapsed":11,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","\n","# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n","pyLDAvis.display(vis)\n","\n","# Save the visualization as an HTML file (optional)\n","pyLDAvis.save_html(vis, 'lda_visualization.html')\n","\n","# Show the visualization in a matplotlib figure (optional)\n","plt.figure(figsize=(8, 4))\n","pyLDAvis.display(vis)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hrq2X0NjT88S","executionInfo":{"status":"aborted","timestamp":1718001008218,"user_tz":-480,"elapsed":11,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["print(documents[0])"]},{"cell_type":"code","source":["from collections import defaultdict"],"metadata":{"id":"pLb4h7jatfli","executionInfo":{"status":"aborted","timestamp":1718001008219,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dr_5vPEJmUIR","executionInfo":{"status":"aborted","timestamp":1718001008219,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["# Initialize topic terms dictionary\n","topic_terms = defaultdict(list)\n","\n","# Initialize topic percentage dictionary\n","topic_percentage_total = defaultdict(float)\n","\n","# Get top terms for each topic and calculate aggregated topic percentages\n","for topic_id in range(lda_model.num_topics):\n","    topic_terms[topic_id] = [term for term, _ in lda_model.show_topic(topic_id)]\n","\n","    # Calculate aggregated topic percentages\n","    for doc_id, doc in enumerate(cleaned_documents):\n","        topic_distribution = lda_model.get_document_topics(corpus[doc_id])\n","        for topic, percentage in topic_distribution:\n","            if topic == topic_id:\n","                topic_percentage_total[topic_id] += percentage / len(cleaned_documents)\n","\n","# Plot the aggregated topic percentages\n","topics = list(topic_percentage_total.keys())\n","percentages = list(topic_percentage_total.values())\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(topics, percentages, color='skyblue')\n","plt.xlabel('Topic')\n","plt.ylabel('Average Percentage')\n","plt.title('Average Topic Percentage Across Documents')\n","\n","# Display the top terms for each topic\n","for topic_id, terms in topic_terms.items():\n","    plt.text(topic_id, topic_percentage_total[topic_id] + 0.5, ', '.join(terms), ha='center', va='bottom', rotation=90)\n","\n","plt.xticks(range(len(topics)), [f\"Topic {topic_id}\" for topic_id in topics])\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaJip-NVs_Zz","executionInfo":{"status":"aborted","timestamp":1718001008219,"user_tz":-480,"elapsed":11,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVsvOosPsyES","executionInfo":{"status":"aborted","timestamp":1718001008219,"user_tz":-480,"elapsed":11,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["for topic_id, topic in enumerate(lda_model.print_topics(num_topics=10, num_words=20)):\n","    topic_words = \" \".join([word.split(\"*\")[1].strip() for word in topic[1].split(\" + \")])\n","    wordcloud = WordCloud(width=800, height=800, random_state=21, max_font_size=110).generate(topic_words)\n","    plt.figure()\n","    plt.imshow(wordcloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.title(\"Topic: {}\".format(topic_id))\n","    plt.show()"]},{"cell_type":"code","source":["print(documents[40])"],"metadata":{"id":"IW1JevPxuO90","executionInfo":{"status":"aborted","timestamp":1718001008220,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dTvf2eVMTmK","executionInfo":{"status":"aborted","timestamp":1718001008220,"user_tz":-480,"elapsed":12,"user":{"displayName":"MUHAMMAD AJRUL AMIN BIN MOHD ZAIDI","userId":"03324032608270041025"}}},"outputs":[],"source":["from collections import defaultdict\n","\n","# Get topic distribution for the document\n","topic_distribution = lda_model.get_document_topics(corpus[40])\n","\n","# Convert topic distribution to percentage\n","topic_percentage = defaultdict(float)\n","for topic, percentage in topic_distribution:\n","    topic_percentage[topic] = percentage * 100\n","\n","# Plot the topic percentages\n","topics = list(topic_percentage.keys())\n","percentages = list(topic_percentage.values())\n","\n","plt.bar(topics, percentages)\n","plt.xlabel('Topic')\n","plt.ylabel('Percentage')\n","plt.title('Topic Percentage in Document')\n","plt.xticks(range(len(topics)), topics)\n","plt.show()"]},{"cell_type":"markdown","source":["Name: Muhammad Izzul Islam Bin Faisal, Muhammad Ajrul Amin Bin Mohd Zaidi\n","\n","Student Number: A200363, A194789\n","\n","\n","# Question\n","\n","In this activity, you will fine-tune the LDA parameters by adjusting this parameters:\n","\n","a) alpha : 0.25, 0.5, 0.75, 1.0\n","\n","b) num_topics = 3, 6, 9, 12\n","\n","c) iterations = 25, 50, 75, 100\n","\n","d) passes = 2, 4, 6, 8\n","\n","1. Run 2 LDA models - each with different set of parameters.\n","\n","  1a) alpha = 0.25, num_topics = 7, iteration = 100, passes = 6\n","\n","  1b) alpha = 1.0, num_topics = 7, iteration = 100, passes = 6\n","\n","  2a) alpha = 1.0, num_topics = 9, iteration = 50, passes = 8\n","\n","  2b) alpha = 1.0, num_topics = 12, iteration = 50, passes = 8\n","  \n","\n","\n","2. Compare the result of the two models\n","\n","a) Observe the coherence and perplexity score for each model\n","\n","  1a)  \n","\n","    - Coherence Score: 0.4945604234638519\n","\n","    - Perplexity:  -8.2351585916133\n","\n","  1b)\n","  \n","    - Coherence Score: 0.38720069851969047\n","  \n","    - Perplexity:  -8.220805528031194\n","\n","  2a)\n","\n","    - Coherence Score: 0.3962586175679874\n","\n","    - Perplexity:  -8.44691534104617\n","\n","  2b)\n","\n","    - Coherence Score: 0.3914515838525343\n","\n","    - Perplexity:  -8.92480406807112\n","\n","b) Explain how these parameter influenced the topics across documents.\n","\n","alpha:\n","\n","    - lower alpha value means each documents are associated with fewer and more distinct topics. This leads to higher coherence scores and slightly better perplexity, indicating more interpretable and well-defined topics.\n","    - higher alpha value emans that each documents are associated with more overlapping topics. This leads to lower coherence scores and slightly worse perplexity, indicating less distinct and less interpretable topics.\n","\n","num_topics:\n","\n","    - When the number of subjects is reduced (as in Model 2a with 9 topics), each topic becomes larger, covering more general issues. This can result in higher coherence ratings since the themes are less fragmented and the high-scoring words within each subject are more likely to be semantically similar. However, it may fail to catch certain nuances in the data.\n","    - When the number of topics increases (as in Model 2b's 12 topics), the subjects become more detailed, collecting finer features within the data. This can result in poorer coherence scores since the high-scoring words within each subject may be less semantically connected due to the topics' granularity. However, it frequently leads in a better fit (lower perplexity) since the model can detect more particular patterns in the data.\n"],"metadata":{"id":"J3wsj9-iKkXW"}}],"metadata":{"colab":{"provenance":[{"file_id":"1uN7W6QjmsalmU5rQck0CGNjk5x7pFPKK","timestamp":1717996509292},{"file_id":"1xvv-gLgJm0ECVy3FoM6wiWeL87honxV0","timestamp":1717992585936},{"file_id":"1IYQ6WeXka1yH8SNEExE2U4Me5oMjTE0m","timestamp":1717413263278}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}